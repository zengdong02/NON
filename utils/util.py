import torch
import torch.nn.functional as F
import random
import os
import numpy as np


def set_seed(seed: int):
    """
    è®¾ç½®å…¨å±€éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°ã€‚
    è¦†ç›–: python random, numpy, torch (cpu & gpu), cudnn
    """
    random.seed(seed)
    
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    np.random.seed(seed)
    
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def to_device(data, device):
    """
    é€’å½’åœ°å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ (CPU/GPU)ã€‚
    æ”¯æŒ: torch.Tensor, list, tuple, dict, ä»¥åŠå®ç°äº† .to() æ–¹æ³•çš„å¯¹è±¡ (å¦‚ PyG Data)ã€‚
    """
    if isinstance(data, torch.Tensor):
        return data.to(device)
    
    elif isinstance(data, dict):
        return {k: to_device(v, device) for k, v in data.items()}
    
    elif isinstance(data, list):
        return [to_device(v, device) for v in data]
    
    elif isinstance(data, tuple):
        return tuple(to_device(v, device) for v in data)
    
    elif hasattr(data, 'to'):
        return data.to(device)
    
    return data


def get_sparse_eye(num_nodes, device='cpu'):
    """
    ç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º (num_nodes, num_nodes) çš„ç¨€ç–å•ä½çŸ©é˜µ (torch.sparse_coo_tensor)
    
    å‚æ•°:
        num_nodes: èŠ‚ç‚¹æ•°é‡ (çŸ©é˜µå¤§å°)
        device: è®¾å¤‡ ('cpu' æˆ– 'cuda')
    """
    idx = torch.arange(num_nodes, device=device)
    indices = torch.stack([idx, idx], dim=0)

    values = torch.ones(num_nodes, device=device)

    sparse_eye = torch.sparse_coo_tensor(
        indices, 
        values, 
        (num_nodes, num_nodes), 
        device=device
    )
   
    return sparse_eye.coalesce()


def inspect_tensor(name, tensor):
    """è¯¦ç»†æ‰“å° Tensor çš„å…ƒæ•°æ®ä¿¡æ¯"""
    if not torch.is_tensor(tensor):
        print(f"âš ï¸ [{name}] ä¸æ˜¯ Tensorï¼Œç±»å‹æ˜¯: {type(tensor)}")
        if isinstance(tensor, (int, float)):
             print(f"   Value: {tensor}")
        return

    # åŸºç¡€ä¿¡æ¯
    layout_type = "SPARSE" if tensor.is_sparse else "DENSE"
    info = (
        f"ğŸ” [{name}] "
        f"Shape={tuple(tensor.shape)} | "
        f"Type={layout_type} ({tensor.layout}) | "
        f"Device={tensor.device} | "
        f"Dtype={tensor.dtype}"
    )

    # æ•°å€¼å¥åº·æ£€æŸ¥ (NaN/Inf)
    # æ³¨æ„ï¼šç¨€ç–çŸ©é˜µç›´æ¥ç”¨ .any() å¯èƒ½ä¼šæŠ¥é”™æˆ–å¾ˆæ…¢ï¼Œé€šå¸¸åªæ£€æŸ¥ values
    try:
        if tensor.is_sparse:
            values = tensor.values()
            nnz = tensor._nnz()
            info += f" | NNZ={nnz}" # éé›¶å…ƒç´ æ•°é‡
        else:
            values = tensor
        
        has_nan = torch.isnan(values).any().item()
        has_inf = torch.isinf(values).any().item()
        
        if has_nan: info += " | âŒ å« NaN"
        if has_inf: info += " | âŒ å« Inf"
        
        # æ‰“å°éƒ¨åˆ†ç»Ÿè®¡å€¼å¸®åŠ©åˆ¤æ–­é‡çº§
        if values.numel() > 0 and not has_nan:
             info += f" | Min={values.min().item():.4f}, Max={values.max().item():.4f}"

    except Exception as e:
        info += f" | (æ•°å€¼æ£€æŸ¥å¤±è´¥: {e})"

    print(info)


def sim_con(z_1, z_2, temperature):
    """
    è®¡ç®—ä¸¤ä¸ªç‰¹å¾çŸ©é˜µä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ (å…¨ç¨ å¯†è®¡ç®—ç‰ˆæœ¬)ã€‚
    
    Args:
        z_1: (N, D) Tensor, èŠ‚ç‚¹ç‰¹å¾ 1
        z_2: (N, D) Tensor, èŠ‚ç‚¹ç‰¹å¾ 2
        temperature: float, æ¸©åº¦ç³»æ•° (ä¾‹å¦‚ 0.2)
        
    Returns:
        logits: (N, N) Dense Tensor, ç›¸ä¼¼åº¦çŸ©é˜µ (æœªç»è¿‡ exp)
    """
    # 1. å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè¾“å…¥æ˜¯ç¨€ç–çŸ©é˜µï¼Œå¼ºåˆ¶è½¬ä¸ºç¨ å¯†
    #    è¿™æ ·èƒ½ä¿è¯åç»­çš„çŸ©é˜µä¹˜æ³•ä½¿ç”¨é’ˆå¯¹ç¨ å¯†ä¼˜åŒ–çš„ torch.mm
    if z_1.is_sparse:
        z_1 = z_1.to_dense()
    if z_2.is_sparse:
        z_2 = z_2.to_dense()

    # 2. L2 å½’ä¸€åŒ– (L2 Normalization)
    #    ä½™å¼¦ç›¸ä¼¼åº¦ = (A . B) / (|A| * |B|)
    #    å…ˆå¯¹å‘é‡åšå½’ä¸€åŒ–ï¼Œä¹‹ååªéœ€è¦åšç‚¹ç§¯å³å¯
    z_1_norm = F.normalize(z_1, dim=1)
    z_2_norm = F.normalize(z_2, dim=1)
    
    # 3. çŸ©é˜µä¹˜æ³• (Matrix Multiplication)
    #    (N, D) @ (D, N) -> (N, N)
    #    ç»“æœèŒƒå›´é€šå¸¸åœ¨ [-1/temp, 1/temp] ä¹‹é—´
    similarity = torch.mm(z_1_norm, z_2_norm.t())
    
    return similarity / temperature

def calc_lower_bound(z_1, z_2, pos, temperature=0.2):
    """
    æ–¹æ³• 1: å…¨ç¨ å¯†è®¡ç®— (é€‚ç”¨äº N < 10000 çš„åœºæ™¯)
    ä¸ç®¡è¾“å…¥æ˜¯ Sparse è¿˜æ˜¯ Denseï¼Œå†…éƒ¨ç»Ÿä¸€è½¬ä¸º Dense è¿ç®—ï¼Œå½»åº•æœç» Sparse ç®—å­æŠ¥é”™ã€‚
    """
    EOS = 1e-10
    
    # 1. ç»Ÿä¸€è½¬ä¸º Denseï¼Œç¡®ä¿è®¾å¤‡ä¸€è‡´
    #    å³ä½¿ pos æ˜¯ sparseï¼Œto_dense() åä¹Ÿå°± 183x183ï¼Œéå¸¸å°
    z_1 = z_1.to_dense() if z_1.is_sparse else z_1
    z_2 = z_2.to_dense() if z_2.is_sparse else z_2
    pos = pos.to_dense() if pos.is_sparse else pos
    
    # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡
    if pos.device != z_1.device:
        pos = pos.to(z_1.device)

    # 2. è®¡ç®—ç›¸ä¼¼åº¦ (ç»“æœå¿…ä¸º Dense)
    #    sim_con å†…éƒ¨å¯ä»¥æ˜¯ç®€å•çš„ (z1 @ z2.T) / temp
    sim_matrix = torch.exp(sim_con(z_1, z_2, temperature))

    # 3. Lori 1 (è¡Œå½’ä¸€åŒ–)
    #    Dense / Dense -> Broadcasting å®Œç¾æ”¯æŒ
    row_sum = sim_matrix.sum(dim=1, keepdim=True) + EOS
    prob_1 = sim_matrix / row_sum
    
    #    element-wise ä¹˜æ³• -> æ±‚å’Œ -> log
    lori_1 = -torch.log(torch.clamp(prob_1.mul(pos).sum(dim=-1), min=EOS)).mean()

    # 4. Lori 2 (åˆ—å½’ä¸€åŒ–)
    col_sum = sim_matrix.sum(dim=0, keepdim=True) + EOS
    prob_2 = sim_matrix / col_sum
    
    #    æ³¨æ„ï¼šè¿™é‡Œ prob_2 éœ€è¦è½¬ç½®æ¥åŒ¹é… pos çš„è¡Œ
    #    æˆ–è€…ï¼špos.t() * prob_2 (å–å†³äºä½ çš„æ•°å­¦å®šä¹‰ï¼Œé€šå¸¸æ˜¯å¯¹ç§°çš„)
    #    æ ¹æ®ä½ ä¹‹å‰çš„ä»£ç é€»è¾‘ prob_2 = prob_2.t()
    prob_2 = prob_2.t()
    
    lori_2 = -torch.log(torch.clamp(prob_2.mul(pos).sum(dim=-1), min=EOS)).mean()

    return (lori_1 + lori_2) / 2


def knn_fast(X, k, b):
    device = X.device 
    
    X = F.normalize(X, dim=1, p=2)
    index = 0
    num_nodes = X.shape[0]
    
    values = torch.zeros(num_nodes * (k + 1), device=device)
    rows = torch.zeros(num_nodes * (k + 1), device=device)
    cols = torch.zeros(num_nodes * (k + 1), device=device)
    norm_row = torch.zeros(num_nodes, device=device)
    norm_col = torch.zeros(num_nodes, device=device)
    
    while index < num_nodes:
        if (index + b) > num_nodes:
            end = num_nodes
        else:
            end = index + b
            
        sub_tensor = X[index:index + b]
        similarities = torch.mm(sub_tensor, X.t())
        vals, inds = similarities.topk(k=k + 1, dim=-1)
        vals = torch.clamp(vals, min=0.0)
        
        start_idx = index * (k + 1)
        end_idx = end * (k + 1)
        values[start_idx:end_idx] = vals.view(-1)
        cols[start_idx:end_idx] = inds.view(-1).float()
        
        current_rows = torch.arange(index, end, device=device).view(-1, 1).repeat(1, k + 1).view(-1)
        rows[start_idx:end_idx] = current_rows.float()
        
        norm_row[index: end] = torch.sum(vals, dim=1)
        norm_col.index_add_(-1, inds.view(-1), vals.view(-1))
        
        index += b
        
    norm = norm_row + norm_col
    rows = rows.long()
    cols = cols.long()

    EOS = 1e-10
    norm = torch.clamp(norm, min=EOS)
    
    values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))
    
    return rows, cols, values



def get_k_shot_split(labels, k_shot, num_classes, seed):
    """
    æ ¹æ®ç»™å®šçš„ seed å’Œ k_shot åˆ’åˆ† Support/Query é›†åˆ
    
    Args:
        labels (Tensor): æ‰€æœ‰èŠ‚ç‚¹çš„æ ‡ç­¾ï¼ŒShape [N]
        k_shot (int): æ¯ä¸ªç±»åˆ«é€‰å¤šå°‘ä¸ªæ ·æœ¬ä½œä¸º Support
        num_classes (int): æ€»ç±»åˆ«æ•°
        seed (int): éšæœºç§å­
    
    Returns:
        support_idx (Tensor): Support Set çš„èŠ‚ç‚¹ç´¢å¼•
        query_idx (Tensor): Query Set çš„èŠ‚ç‚¹ç´¢å¼•
    """
    # 1. åˆ›å»ºç‹¬ç«‹çš„éšæœºçŠ¶æ€ç”Ÿæˆå™¨ (ä¸å½±å“å…¨å±€éšæœºçŠ¶æ€)
    rng = np.random.RandomState(seed)
    
    support_indices = []
    query_indices = []
    
    # ç¡®ä¿ labels åœ¨ CPU ä¸Šä»¥ä¾¿ numpy å¤„ç†
    labels_np = labels.cpu().numpy()
    
    for c in range(num_classes):
        # 2. è·å–å½“å‰ç±»åˆ« c çš„æ‰€æœ‰èŠ‚ç‚¹ç´¢å¼•
        # np.where è¿”å›çš„æ˜¯ tupleï¼Œå– [0]
        class_idx = np.where(labels_np == c)[0]
        
        # 3. æ£€æŸ¥æ ·æœ¬å¤Ÿä¸å¤Ÿ
        if len(class_idx) < k_shot:
            print(f"Warning: Class {c} has only {len(class_idx)} samples, fewer than k={k_shot}. Using all for support.")
            selected = class_idx
            remaining = []
        else:
            # 4. éšæœºé€‰æ‹© k_shot ä¸ªä½œä¸º Support
            selected = rng.choice(class_idx, size=k_shot, replace=False)
            # 5. å‰©ä½™çš„ä½œä¸º Query (ä½¿ç”¨ setdiff1d æ‰¾å‡ºå·®é›†)
            remaining = np.setdiff1d(class_idx, selected)
            
        support_indices.append(selected)
        query_indices.append(remaining)
    
    # 6. æ‹¼æ¥å¹¶è½¬æ¢ä¸º Tensor
    support_idx = np.concatenate(support_indices)
    query_idx = np.concatenate(query_indices)
    
    return torch.from_numpy(support_idx).long(), torch.from_numpy(query_idx).long()


def build_prototypes(embeddings, labels, support_idx, num_classes):
    prototypes = []
    for c in range(num_classes):
        # æ‰¾å‡ºå½“å‰ç±»åˆ« c åœ¨ support set ä¸­çš„ä½ç½®
        c_mask = (labels[support_idx] == c)
        c_emb = embeddings[support_idx][c_mask]
        
        if c_emb.size(0) == 0:
            # æå°‘æ•°æƒ…å†µä¸‹çš„é˜²å¾¡æœºåˆ¶
            proto = torch.zeros(1, embeddings.size(1)).to(embeddings.device)
        else:
            proto = c_emb.mean(dim=0, keepdim=True)
        prototypes.append(proto)
    return torch.cat(prototypes, dim=0) # [Num_Classes, Dim]

# --- è¾…åŠ©å‡½æ•°ï¼šåŸå‹ Loss ---
def prototypical_loss(prototypes, queries, targets):
    # dists: [Batch, Num_Classes]
    dists = torch.cdist(queries, prototypes, p=2) 
    return F.cross_entropy(-dists, targets)